# ðŸ› ï¸ Portfolio Projects â€” Detailed Build Specs

> **Rule**: For every ðŸŸ¢ MUST video you watch, spend **2x the time building**. These 6 projects ARE your resume.

> [!IMPORTANT]
> Projects **stack on each other**. By Project 6, you're deploying Projects 3+4+5 as one production system. Treat them as one evolving repo.

---

## Project 1: Professional Python Scaffold *(Module 1, Week 1â€“2)*

### What You're Building
A reusable project template that demonstrates modern Python best practices â€” the foundation every other project will use.

### Tech Stack
`Python 3.12` Â· `uv` Â· `python-dotenv` Â· `asyncio` Â· `mypy` Â· `ruff`

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **uv project setup** | `uv init`, `pyproject.toml`, lockfile, `.python-version` |
| 2 | **Type-hinted async code** | Write 3â€“4 async functions that call a free API (e.g., weather, jokes) with full type hints |
| 3 | **Pattern matching** | Use `match/case` to route different API response types |
| 4 | **Secrets management** | `.env` file for API keys, loaded via `python-dotenv`, never committed |
| 5 | **Linting + type checking** | `ruff` for linting, `mypy --strict` passes with zero errors |
| 6 | **Clean `.gitignore`** | Exclude `.env`, `__pycache__`, `.venv`, `uv.lock` |

### Folder Structure
```
ai-agent-scaffold/
â”œâ”€â”€ pyproject.toml          # uv project config
â”œâ”€â”€ .python-version         # Python 3.12
â”œâ”€â”€ .env.example            # Template (no real keys)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py             # Entry point with async main()
â”‚   â”œâ”€â”€ api_client.py       # Async API calls with type hints
â”‚   â””â”€â”€ config.py           # Load .env, export settings
â””â”€â”€ README.md
```

### Interview Talking Points
- *"I use uv instead of pip because it's 10â€“100x faster and handles Python versions + virtualenvs + dependencies in one tool"*
- *"Every function is type-hinted and passes mypy strict mode â€” this matters for PydanticAI later"*

---

## Project 2: Customer Support Triage Agent *(Module 2, Week 3â€“4)*

### What You're Building
A multi-agent customer support system that routes customer queries to the right specialist agent, with safety guardrails.

### Tech Stack
`PydanticAI` Â· `OpenAI Agents SDK` Â· `Pydantic V2` Â· `uv` Â· any LLM API (OpenAI / Gemini / Groq)

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **PydanticAI version** | Build a single agent with structured output: classifies queries into `billing`, `technical`, `sales` using Pydantic models |
| 2 | **Structured outputs** | Define response models: `TriageResult(category, urgency, summary, suggested_action)` |
| 3 | **Tool injection** | Agent has tools: `lookup_order(order_id)`, `check_account_status(email)` â€” return mock data |
| 4 | **OpenAI SDK rebuild** | Rebuild the same system using OpenAI Agents SDK with 3 specialist agents |
| 5 | **Handoffs** | Triage Agent â†’ hands off to `BillingAgent`, `TechSupportAgent`, or `SalesAgent` |
| 6 | **Guardrails** | Input guardrail: block profanity/PII. Output guardrail: ensure response doesn't promise refunds without approval |
| 7 | **Compare both** | Write a short `COMPARISON.md` explaining PydanticAI vs SDK trade-offs you discovered |

### Folder Structure
```
customer-triage-agent/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pydantic_version/
â”‚   â”‚   â”œâ”€â”€ agent.py         # PydanticAI agent
â”‚   â”‚   â”œâ”€â”€ models.py        # Pydantic response models
â”‚   â”‚   â””â”€â”€ tools.py         # Tool functions
â”‚   â”œâ”€â”€ sdk_version/
â”‚   â”‚   â”œâ”€â”€ triage_agent.py  # Router agent
â”‚   â”‚   â”œâ”€â”€ billing_agent.py
â”‚   â”‚   â”œâ”€â”€ tech_agent.py
â”‚   â”‚   â”œâ”€â”€ sales_agent.py
â”‚   â”‚   â””â”€â”€ guardrails.py    # Input + output guardrails
â”‚   â””â”€â”€ run.py               # CLI to test both versions
â”œâ”€â”€ COMPARISON.md
â””â”€â”€ README.md
```

### Interview Talking Points
- *"I built the same system twice â€” once with PydanticAI, once with OpenAI SDK â€” so I can articulate trade-offs"*
- *"The guardrails prevent the agent from making financial commitments without human approval"*
- *"I chose PydanticAI for type-safety in production, SDK for rapid prototyping"*

---

## Project 3: Research & Report Pipeline *(Module 3, Week 5â€“6)*

### What You're Building
A multi-agent pipeline that takes a topic, researches it from the web, analyzes findings, and generates a structured report. This is the **core project** â€” Projects 4, 5, and 6 build on it.

### Tech Stack
`LangGraph` Â· `CrewAI` Â· `Tavily Search API` (free tier) Â· `PydanticAI` or SDK agents

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **LangGraph state graph** | Define a graph with 3 nodes: `research` â†’ `analyze` â†’ `write` |
| 2 | **Research Agent** | Uses Tavily Search API to find 5â€“10 relevant sources on a given topic |
| 3 | **Analyst Agent** | Takes raw search results, extracts key facts, identifies contradictions |
| 4 | **Writer Agent** | Generates a structured Markdown report with citations |
| 5 | **State persistence** | Use LangGraph checkpointing â€” pipeline can resume if interrupted |
| 6 | **Conditional routing** | If Research Agent finds < 3 sources, it retries with broader query |
| 7 | **CrewAI version (optional)** | Rebuild the same pipeline using CrewAI Flows for comparison |
| 8 | **Human-in-the-loop** | Add a breakpoint after `analyze` â€” user can approve/reject before writing |

### Folder Structure
```
research-pipeline/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ graph.py             # LangGraph state graph definition
â”‚   â”œâ”€â”€ state.py             # State schema (Pydantic)
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ researcher.py    # Web search agent
â”‚   â”‚   â”œâ”€â”€ analyst.py       # Data processing agent
â”‚   â”‚   â””â”€â”€ writer.py        # Report generation agent
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â””â”€â”€ search.py        # Tavily search tool
â”‚   â””â”€â”€ run.py               # CLI entry point
â”œâ”€â”€ output/                  # Generated reports go here
â””â”€â”€ README.md
```

### Interview Talking Points
- *"I chose LangGraph over CrewAI because I needed fine-grained control over state and conditional routing"*
- *"The pipeline has checkpointing â€” if the LLM API times out mid-analysis, it resumes from where it stopped"*
- *"I added human-in-the-loop at the analysis stage because I learned from Reddit that fully autonomous agents fail silently"*

---

## Project 4: AI Knowledge Base with MCP + RAG *(Module 4, Week 7â€“8)*

### What You're Building
A personal knowledge base agent that answers questions using your own documents, powered by both vector search AND graph relationships.

### Tech Stack
`FastMCP` Â· `PostgreSQL + pgvector` Â· `Neo4j` Â· `Qdrant` (optional) Â· `LangGraph` (from Project 3)

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **MCP Server** | Build a FastMCP server exposing 3 tools: `search_docs`, `get_related`, `summarize` |
| 2 | **Document ingestion** | Script to chunk Markdown/PDF files, generate embeddings (OpenAI or local), store in pgvector |
| 3 | **Vector search** | `search_docs` tool queries pgvector using cosine similarity, returns top-5 chunks |
| 4 | **Graph relationships** | Store document â†’ section â†’ concept relationships in Neo4j |
| 5 | **Graph-enhanced RAG** | When user asks a question: vector search finds relevant chunks + Neo4j finds related concepts = richer context |
| 6 | **Agent integration** | Connect the MCP server to your Project 3 agent system â€” it now has "memory" |
| 7 | **Hybrid retrieval** | Combine vector similarity + graph traversal results before sending to LLM |

### Folder Structure
```
knowledge-base-agent/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml       # PostgreSQL + Neo4j containers
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_server/
â”‚   â”‚   â”œâ”€â”€ server.py        # FastMCP server definition
â”‚   â”‚   â”œâ”€â”€ tools.py         # search_docs, get_related, summarize
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ chunker.py       # Split docs into chunks
â”‚   â”‚   â”œâ”€â”€ embedder.py      # Generate embeddings
â”‚   â”‚   â””â”€â”€ graph_builder.py # Build Neo4j relationships
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ vector_search.py # pgvector queries
â”‚   â”‚   â””â”€â”€ graph_search.py  # Neo4j traversal
â”‚   â””â”€â”€ run.py
â”œâ”€â”€ data/                    # Sample docs to ingest
â””â”€â”€ README.md
```

### Interview Talking Points
- *"I built an MCP server so any MCP-compatible client (Claude, Cursor, etc.) can use my knowledge base"*
- *"Pure vector search missed relationships between concepts â€” adding Neo4j increased answer relevance by ~30% in my tests"*
- *"I used pgvector instead of a dedicated vector DB because my data fits in PostgreSQL and I avoid adding infrastructure"*

---

## Project 5: Observability & Eval Suite *(Module 5, Week 9â€“10)*

### What You're Building
Add production-grade monitoring and automated testing to your Project 3 pipeline. This turns a "demo" into a "production-ready system."

### Tech Stack
`LangSmith` Â· `Evals (LLM-as-Judge)` Â· project 3 pipeline Â· `pytest`

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **LangSmith integration** | Trace every LLM call, tool invocation, and handoff in Project 3 pipeline |
| 2 | **Custom trace metadata** | Tag traces with: `run_id`, `topic`, `model`, `total_cost`, `latency` |
| 3 | **Golden dataset** | Create 20 test cases: `{topic, expected_sections, expected_facts, quality_criteria}` |
| 4 | **LLM-as-Judge eval** | For each generated report, a judge LLM scores: `factual_accuracy`, `completeness`, `coherence`, `citation_quality` |
| 5 | **Scoring rubric** | Use named categories not numbers: `excellent`, `acceptable`, `needs_improvement`, `failed` |
| 6 | **Regression tests** | `pytest` suite that runs 5 core test cases, asserts quality scores â‰¥ `acceptable` |
| 7 | **Dashboard metrics** | Track across runs: avg latency, cost/run, success rate, common failure categories |

### Folder Structure
```
# Add to your research-pipeline/ project:
research-pipeline/
â”œâ”€â”€ ...existing files...
â”œâ”€â”€ evals/
â”‚   â”œâ”€â”€ golden_dataset.json  # 20 test cases
â”‚   â”œâ”€â”€ judge.py             # LLM-as-Judge scoring
â”‚   â”œâ”€â”€ rubric.py            # Named category rubrics
â”‚   â””â”€â”€ run_evals.py         # Run all evals, output report
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py     # pytest regression tests
â””â”€â”€ observability/
    â”œâ”€â”€ tracing.py           # LangSmith setup + custom metadata
    â””â”€â”€ dashboard.py         # Metrics aggregation
```

### Interview Talking Points
- *"I test my agents the same way you'd test software â€” golden datasets, regression suites, and automated scoring"*
- *"I use named categories instead of 1-10 scales because Reddit discussions showed LLMs are better at categorical judgments"*
- *"LangSmith tracing helped me find that 40% of my latency was in the research step â€” I optimized by caching search results"*

---

## Project 6: Production Deployment *(Module 6, Week 11â€“12)*

### What You're Building
Deploy your full agent system (Projects 3+4+5) as a production API with caching, containerization, and cloud hosting. **This is your capstone.**

### Tech Stack
`FastAPI` Â· `Docker + Docker Compose` Â· `Redis` Â· `Modal` or `Cloud Run` Â· GitHub Actions CI/CD

### Features to Build

| # | Feature | Details |
|---|---------|---------|
| 1 | **FastAPI wrapper** | REST API: `POST /research` (start pipeline), `GET /status/{run_id}`, `GET /report/{run_id}` |
| 2 | **WebSocket streaming** | Stream agent progress to client in real-time: "Researchingâ€¦ 3/10 sources found" |
| 3 | **Redis caching** | Cache search results (TTL: 1hr) + cache completed reports (TTL: 24hr) |
| 4 | **Redis agent memory** | Store conversation history per session in Redis |
| 5 | **Dockerize everything** | `Dockerfile` for the app + `docker-compose.yml` with FastAPI + Redis + PostgreSQL + Neo4j |
| 6 | **Health checks** | `/health` endpoint checking all services (Redis, DB, LLM API connectivity) |
| 7 | **Deploy to cloud** | Pick one: Modal (for GPU) or Cloud Run (for API). Deploy with one command |
| 8 | **CI/CD** | GitHub Actions: on push â†’ run evals â†’ if pass â†’ deploy |
| 9 | **README as portfolio** | Screenshot/GIF of the API working, architecture diagram, tech stack badges |

### Folder Structure
```
ai-agent-production/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml       # FastAPI + Redis + PostgreSQL + Neo4j
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml       # CI/CD pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI app
â”‚   â”‚   â”œâ”€â”€ routes.py        # /research, /status, /report
â”‚   â”‚   â”œâ”€â”€ websocket.py     # Real-time streaming
â”‚   â”‚   â””â”€â”€ middleware.py    # Auth, rate limiting, CORS
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ redis_client.py  # Redis caching + session memory
â”‚   â”œâ”€â”€ agents/              # From Project 3
â”‚   â”œâ”€â”€ mcp_server/          # From Project 4
â”‚   â”œâ”€â”€ evals/               # From Project 5
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ modal_deploy.py      # Modal deployment script
â”‚   â””â”€â”€ cloudrun.yaml        # Cloud Run config
â””â”€â”€ README.md                # â­ This IS your portfolio piece
```

### Interview Talking Points
- *"I deployed a multi-agent system with 4 services (API + Redis + Postgres + Neo4j) using Docker Compose, then pushed to Cloud Run"*
- *"Redis semantic caching cut my LLM costs by 35% by avoiding duplicate queries"*
- *"I have CI/CD â€” every push runs my eval suite and only deploys if all 20 test cases pass"*
- *"The API streams agent progress via WebSocket so users see what's happening, not just a loading spinner"*

---

## ðŸ—ºï¸ How the projects connect

```
Project 1 (scaffold) â”€â”€sets upâ”€â”€â–¶ Every other project

Project 2 (triage agent) â”€â”€teaches youâ”€â”€â–¶ Agent patterns for Projects 3â€“6

Project 3 (pipeline) â”€â”€is enhanced byâ”€â”€â–¶ Project 4 (knowledge base)
                      â”€â”€is tested byâ”€â”€â–¶ Project 5 (evals)
                      â”€â”€is deployed byâ”€â”€â–¶ Project 6 (production)

Project 6 = Project 3 + 4 + 5 deployed together ðŸš€
```

> [!TIP]
> **GitHub tip**: Create ONE repo called `ai-agent-system` and evolve it across all 6 projects. Interviewers love seeing a commit history that shows progressive learning.
